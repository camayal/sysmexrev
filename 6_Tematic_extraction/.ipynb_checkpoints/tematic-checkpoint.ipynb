{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition function to search tematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition for tematic addition\n",
    "def add_tematic_column(file, regex, fields_to_search, search_title, overwrite=True, sep=\",\", index_col=0):\n",
    "    \"\"\"\n",
    "    Open database and search reges in fields_to_search and append column\n",
    "    \"\"\"\n",
    "    # load database\n",
    "    df = pd.read_csv(file, low_memory = True, delimiter=sep, index_col=index_col)\n",
    "\n",
    "    # code title\n",
    "    search_title = f\"t.{search_title}\"\n",
    "    search_title= re.sub(\"\\s\", \"_\", search_title)\n",
    "\n",
    "    # create empty column for result\n",
    "    df[search_title] = np.nan\n",
    "\n",
    "    # iterate row by row\n",
    "    for index, row in df.iterrows(): \n",
    "        # search every pattern in title\n",
    "        for field in fields_to_search:\n",
    "            try: \n",
    "                if re.search(regex, row[field], re.IGNORECASE):\n",
    "                    #add Auto in the cell to indicate this part of the search is true\n",
    "                    df.loc[index, search_title] = \"1\"\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "    if overwrite:\n",
    "        df.to_csv(file)\n",
    "        return f\"Found {df[df[search_title].notnull()][search_title].shape[0]} for {search_title} regex ({regex}) in {fields_to_search}, {file} overwrote!\"\n",
    "    else:\n",
    "        return df[df[search_title].notnull()][search_title]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of some topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of markers for detec sanger, nuclear, and chloroplast info\n",
    "nuclear_markers = \"Internal transcribed spacer|ITS1|ITS2|ETS|nrITS|18S|59S|5.8s|GBSSI|COSII|25S|Ypr10|pgiC|ETS|ADH2F|ADH3R|Aat|Skdh|Pgi.?2|Tpi.?1|Tpi.?2|AFLP\"\n",
    "chl_markers = \"trnL|trnG|trnR|trnT|trnS|trnF|trnH|trnQ|trnD|trnY|trnE|atpB|rubisco|rbcL|psbA|matk|psbJ|petA|ndhF|psaA|ycf3|ycf4|rpl16|trnSfM|trnfM|rps4|cemA|psbM|trnDGUC\"\n",
    "\n",
    "\n",
    "               \n",
    "# set catagories of search\n",
    "categories = {\n",
    "    \"Mexico\": \"(?<!new)(?<!nuevo)\\s+m.[xj]ic\",\n",
    "    \"Morphology\": \"morpholog|morfolog\",\n",
    "    \"First gen.\": f\"{nuclear_markers}|{chl_markers}| sanger | marker| nuclear region| chloroplast region|regi.n.*? nucr|regi.n.*? cloro| spacer|AFLP|Microsat.ll?ite\",\n",
    "    \"Second gen.\": \"radseq|angiosperm353|hybseq|454|illumina|iontorrent|whole genome|plastome\",\n",
    "    \"Third gen.\": \"pacbio|nanopore\",\n",
    "    \"Nuclear info\": f\"nuclear|{nuclear_markers}\",\n",
    "    \"Chloroplast info\": f\"c.?lorop|{chl_markers}\",\n",
    "    \"Parsimony\": \"parsimon\",\n",
    "    \"Maximum likelihood\": \" ML |maximum likelihood|verosimilitud\",\n",
    "    \"Bayesian\": \"bayes| beast\",\n",
    "    \"MSC\": \" astral | astrid | bpp |coalescent\",\n",
    "    \"New species\": \"new species|nueva. especie\",\n",
    "    \"New genus\": \"new genus|nueva.? g.nero\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add tematics in previous topics (major topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over ALL categories. Replace previous results!\r\n",
    "for category in categories:\r\n",
    "    add_tematic_column(\"bare_final_database.csv\", categories[category], [\"c.abstract\", \"c.title\"], category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For individual tematics use add_tematic_column method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual families extraction -- Should be replaced by another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get families from abstracts\n",
    "\n",
    "file = \"bare_final_database.csv\"\n",
    "df = pd.read_csv(file, low_memory = True, index_col=0)\n",
    "\n",
    "fields_to_search = [\"c.abstract\", \"c.title\"]\n",
    "\n",
    "#Load list of families by major groups\n",
    "with  open(\"../Data_ThePlantList/Angiosperms.txt\", \"r\") as txtfile: \n",
    "    angiosperms = [line.strip() for line in txtfile]\n",
    "    \n",
    "with  open(\"../Data_ThePlantList/Bryophytes.txt\", \"r\") as txtfile: \n",
    "    bryophytes = [line.strip() for line in txtfile]\n",
    "    \n",
    "with  open(\"../Data_ThePlantList/Gymnosperms.txt\", \"r\") as txtfile: \n",
    "    gymnosperms = [line.strip() for line in txtfile]\n",
    "    \n",
    "with  open(\"../Data_ThePlantList/Pteridophytes.txt\", \"r\") as txtfile: \n",
    "    pteridophytes = [line.strip() for line in txtfile]\n",
    "\n",
    "all_families = angiosperms + bryophytes + gymnosperms + pteridophytes\n",
    "\n",
    "for index, row in df.iterrows(): \n",
    "    families = []\n",
    "    groups = []\n",
    "    for field in fields_to_search:\n",
    "        try:\n",
    "#             [families.append(i.lower()) for i in re.findall(\"[A-Za-z]+(?<=ceae)\", row[field], re.IGNORECASE) if i not in families]\n",
    "            for family in re.findall(\"[A-Za-z]+(?<=ceae)\", row[field], re.IGNORECASE):\n",
    "                family_to_add = \"\"\n",
    "                #check family and correct\n",
    "                family_corrected = [s for s in all_families if fuzz.token_sort_ratio(s, family) >= 80]\n",
    "                if len(family_corrected) == 1:\n",
    "                    family_to_add = family_corrected[0]\n",
    "#                     print(family, \"corrected\", family_corrected[0])\n",
    "                else:                    \n",
    "                    family_to_add = family\n",
    "#                     print(family, \"no corrected\")\n",
    "                \n",
    "                if family_to_add not in families:\n",
    "                    families.append(family_to_add.lower())\n",
    "                \n",
    "                \n",
    "                #check group\n",
    "                if family_to_add in angiosperms:  df.loc[index, \"t.angiosperms\"] = 1\n",
    "                elif family_to_add in bryophytes: df.loc[index, \"t.bryophytes\"] = 1\n",
    "                elif family_to_add in gymnosperms: df.loc[index, \"t.gymnosperms\"] = 1\n",
    "                elif family_to_add in pteridophytes: df.loc[index, \"t.pteridophytes\"] = 1\n",
    "                \n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    #add to df\n",
    "    list_of_families = \",\".join(set(families))\n",
    "    df.loc[index, \"t.families\"] = list_of_families\n",
    "\n",
    "df.to_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add type of journal (open source, scielo), country, Quartil and Impact factor (year based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search journal in SCIMago databases and add info to the main database\n",
    "\n",
    "file = \"bare_final_database.csv\"\n",
    "df = pd.read_csv(file, low_memory = True, index_col=0)\n",
    "\n",
    "#iterarte by year to open only once each big database\n",
    "for year in range(1999, 2021):\n",
    "    \n",
    "    print(year)\n",
    "        \n",
    "    scimagoFULL_df = pd.read_csv(f\"../Data_scimago/all_journals/scimagojr {year}.csv\", low_memory = True, delimiter=\";\", index_col=0, error_bad_lines=False)\n",
    "    scimagoOPEN_df = pd.read_csv(f\"../Data_scimago/opensource_journals/scimagojr {year}.csv\", low_memory = True, delimiter=\";\", index_col=0, error_bad_lines=False)\n",
    "    scimagoSCIELO_df = pd.read_csv(f\"../Data_scimago/scielo_journals/scimagojr {year}.csv\", low_memory = True, delimiter=\";\", index_col=0, error_bad_lines=False)\n",
    "\n",
    "    \n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        journal = row[\"c.journal\"]\n",
    "        paper_year = row[\"c.year\"]\n",
    "\n",
    "        #avoid nulls\n",
    "        if isinstance(journal, str):\n",
    "\n",
    "            #check quartil, impact factor and country from general database\n",
    "            result = scimagoFULL_df[scimagoFULL_df[\"Title\"].str.lower() == journal.lower()]\n",
    "\n",
    "            if result.shape[0] > 0:\n",
    "                # check year for information that change yearly\n",
    "                if paper_year == year:\n",
    "                    if not isinstance(result[\"SJR\"].values[0], float):\n",
    "                        _sjr = float(result[\"SJR\"].values[0].replace(',', '.'))\n",
    "                    else:\n",
    "                        _sjr = result[\"SJR\"].values[0]\n",
    "                    df.loc[index, \"s.sjr\"] = _sjr\n",
    "                    df.loc[index, \"s.q\"] = result[\"SJR Best Quartile\"].values[0]\n",
    "                    df.loc[index, \"s.h\"] =  result[\"H index\"].values[0]\n",
    "                    \n",
    "                #other static information, just added\n",
    "                df.loc[index, \"s.country\"] =  result[\"Country\"].values[0]\n",
    "                df.loc[index, \"s.scimago\"] = 1\n",
    "\n",
    "            #check if exist in opensource databases (THIS COULD BE SENSITIVE TO YEAR TOO, CHECK IF CHANGES)\n",
    "            result = scimagoOPEN_df[scimagoOPEN_df[\"Title\"].str.lower() == journal.lower()]\n",
    "            if result.shape[0] > 0:\n",
    "                df.loc[index, \"s.opensource\"] = 1\n",
    "\n",
    "                \n",
    "            #check if exist in scielo database\n",
    "            result = scimagoSCIELO_df[scimagoSCIELO_df[\"Title\"].str.lower() == journal.lower()]\n",
    "            if result.shape[0] > 0:\n",
    "                df.loc[index, \"s.scielo\"] = 1\n",
    "\n",
    "\n",
    "    \n",
    "    df.to_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add markers used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load database\n",
    "file = \"bare_final_database.csv\"\n",
    "df = pd.read_csv(file, low_memory = True, index_col=0)\n",
    "\n",
    "#define where to search\n",
    "fields_to_search = [\"c.abstract\", \"c.title\"]\n",
    "\n",
    "#define some common markers\n",
    "markers = {\n",
    "    \"18S-26S\": [\"18S-26S\"],\n",
    "    \"18S\": [\"[ (]18S\"],\n",
    "    \"ADH2F-ADH3R\": [\"ADH2F-ADH3R\"],\n",
    "    \"AFLPs\": [\"AFLP\", \"Amplified fragment length polymorphism\"],\n",
    "    \"aptF-aptH\": [\"aptF-aptH\"],\n",
    "    \"atpB-rbcL\": [\"atpB-rbcL\"],\n",
    "    \"atpB\": [\"[ (]atpB\"],\n",
    "    \"clpp\": [\"clpp\"],\n",
    "    \"COSII\": [\"COSII\"],\n",
    "    \"ETS\": [\"ETS\"],\n",
    "    \"GBSSI\": [\"GBSSI\"],\n",
    "    \"ITS\": [\"ITS\", \"ITS1-5.8s-ITS2\", \"ITS1\", \"5.8s\", \"ITS2\", \"Internal transcribed spacer\", \"59S\"],\n",
    "    \"matK\": [\"matK\"],\n",
    "    \"ndhA\": [\"ndhA\"],\n",
    "    \"ndhF\": [\"ndhF\"],\n",
    "    \"petB\": [\"petB\"],\n",
    "    \"pgiC\": [\"pgiC\"],\n",
    "    \"psaA-ycf3\": [\"psaA-ycf3\"],\n",
    "    \"psaC-ndhE\": [\"psaC-ndhE\"],\n",
    "    \"psaJ-rpl33\": [\"psaJ-rpl33\"],\n",
    "    \"psbE-petL\": [\"psbE-petL\"],\n",
    "    \"psbJ‐petA\": [\"psbJ‐petA\"],\n",
    "    \"psbL-trnS\": [\"psbL-trnS[-AUGC]*\"],\n",
    "    \"psbM-trnD\": [\"psbM-trnD[-AUGC]*\"],\n",
    "    \"psbZ-trnG\": [\"psbZ-trnG[-AUGC]*\"],\n",
    "    \"rbcL\": [\"[ (]rbcL\",\"RuBisCO\", \"rubisco\"],\n",
    "    \"rpl12-clpp\": [\"rpl12-clpp\"],\n",
    "    \"rpl32-trnL\": [\"rpl32-trnL[-AUGC]*\"],\n",
    "    \"rpoB-psbZ\": [\"rpoB[-AUGC]*-psbZ[-AUGC]*\", \"BZ\"],\n",
    "    \"rpoB-trnC-GCA\": [\"rpoB-trnC[-AUGC]*\"],\n",
    "    \"rpoC2-rpoC1\": [\"rpoC2-rpoC1\"],\n",
    "    \"rps16-trnQ\": [\"rps16-trnQ[-AUGC]*\"],\n",
    "    \"rps19\": [\"rps19\"],\n",
    "    \"rps2-rpoc2\": [\"rps2-rpoc2\"],\n",
    "    \"rps4-trnS \": [\"rps4-trnS[-AUGC]*\"],\n",
    "    \"trnC-ycf6\": [\"trnC[-AUGC]*-ycf6\"],\n",
    "    \"trnD-psbM\": [\"trnD[-AUGC]*-psbM\"],\n",
    "    \"trnD-trnT\": [\"trnD[-AUGC]*‐trnY[-AUGC]*‐trnE[-AUGC]*‐trnT[-AUGC]*\"],\n",
    "    \"trnD-trnY\": [\"trnD-AUGC]*-trnY-AUGC]*\"],\n",
    "    \"trnD‐trnY\": [\"trnD[-AUGC]*‐trnY[-AUGC]*\"],\n",
    "    \"trnG-trnfM\" : [\"trnG[-AUGC]*-trnfM[-AUGC]*\"],\n",
    "    \"trnH-psbA\": [\"trnH[-AUGC]*-psbA\", \"psbA-trnH[-AUGC]*\"],\n",
    "    \"trnK-rps16\": [\"trnK[-AUGC]*-rps16\"],\n",
    "    \"trnL-trnF\": [\"trnL[-AUGC]*-trnF[-AUGC]*\", \"trnL-F\", \"trnF-trnL\"],\n",
    "    \"trnM-atpE\": [\"trnM[-AUGC]*-atpE\"],\n",
    "    \"trnP-psaJ\": [\"trnP[-AUGC]*-psaJ\"],\n",
    "    \"trnQ-psbK\": [\"trnQ[-AUGC]*-psbK[-AUGC]*\"],\n",
    "    \"trnR-atpA\": [\"trnR[-AUGC]*-atpA\"],\n",
    "    \"trnS-psbZ\": [\"trnS[-AUGC]*-psbZ[-AUGC]*\"],\n",
    "    \"trnS-rps4\": [\"trnS[-AUGC]*-rps4\"],\n",
    "    \"trnS-trnG\": [\"trnS[-AUGC]*-trnG[-AUGC]*\"],\n",
    "    \"trnS‐trnfM \": [\"trnS[-AUGC]*‐trnfM[-AUGC]*\"],\n",
    "    \"trnT-trnE\": [\"trnT[-AUGC]*-trnE[-AUGC]*\", \"trnE[-AUGC]*‐trnT[-AUGC]*\"],\n",
    "    \"trnT-trnF\": [\"trnT[-AUGC]*-trnF[-AUGC]*\", \"trnF[-AUGC]*‐trnT[-AUGC]*\"],\n",
    "    \"trnT-trnL\": [\"trnT[-AUGC]*-trnL[-AUGC]*\", \"trnT-L\"],\n",
    "    \"trnT-trntL\": [\"trnT[-AUGC]*-trntL[-AUGC]*\"],\n",
    "    \"trnY‐trnE\": [\"trnY[-AUGC]*‐trnE[-AUGC]*\"],\n",
    "    \"ycf1\": [\"ycf1\"],\n",
    "    \"ycf15-ycf1\": [\"ycf15-ycf1\"],\n",
    "    \"ycf4-cemA\": [\"ycf4-cemA\"],\n",
    "    \"Ypr10\": [\"Ypr10\"],\n",
    "    \"Microsatellites\": [\"Microsat.ll?ite\"],\n",
    "          }\n",
    "\n",
    "#iterate row by row\n",
    "for index, row in df.iterrows(): \n",
    "    #create empty result for each row\n",
    "    result_markers = []\n",
    "    #iterate for all markers in dictionary\n",
    "    for marker in markers.keys():\n",
    "        \n",
    "        #search in each field defined\n",
    "        for field in fields_to_search:\n",
    "            #replace all ndash and mdash to normal dash en each field\n",
    "            string_field = row[field]\n",
    "            for r in ((\"–\", \"-\"), (\"—\", \"-\")):\n",
    "                if isinstance(string_field, str):\n",
    "                    string_field = string_field.replace(*r)\n",
    "                \n",
    "            #join string_fieldin all synonyms for a given marker\n",
    "            regex_string = \"|\".join(markers[marker])\n",
    "            #search for all elements\n",
    "            try:\n",
    "                search = re.findall(regex_string, string_field)\n",
    "\n",
    "                if search:\n",
    "                    result_markers.append(marker)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    if result_markers:\n",
    "        df.loc[index, \"t.markers\"] = \",\".join(set(result_markers))\n",
    "    else:\n",
    "        df.loc[index, \"t.markers\"] = np.nan\n",
    "\n",
    "                \n",
    "df.to_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add species per study and check mexican endmics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load database\n",
    "file = \"bare_final_database.csv\"\n",
    "df = pd.read_csv(file, low_memory = True, index_col=0)\n",
    "\n",
    "#put endemic species into a list\n",
    "with open(\"../Data_endemicSpeciesMexico/endemics_uniques.txt\", \"r\") as fendemics:\n",
    "    data = fendemics.read()\n",
    "    endemic_mx = data.splitlines()\n",
    "\n",
    "list_endemic_species = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    result_species = []\n",
    "    result_species_endemic = []\n",
    "\n",
    "    #open raw database file\n",
    "    with open(\"../1_Datataxa_extraction/FIRSTTEST.csv\", mode=\"r\") as raw:\n",
    "        #load lines\n",
    "        lines = raw.readlines()\n",
    "        #iterate over each row (aka species)\n",
    "        for line in lines[1:]:\n",
    "            publications = None\n",
    "            species = None\n",
    "            #manually parse each column\n",
    "            cells = line.split(\"\\\",\\\"\")\n",
    "            if len(cells) >= 2:\n",
    "                publications = cells[2].split(\"|\")\n",
    "            if len(cells) >= 5:\n",
    "                species = cells[5][:-2]\n",
    "            \n",
    "            if publications:\n",
    "                #Check if old title is in publications in raw database\n",
    "                if row[\"a.oldtitle\"] in publications:\n",
    "                    result_species.append(species)\n",
    "                    if species in endemic_mx:\n",
    "                        result_species_endemic.append(species)\n",
    "                        list_endemic_species.append(species)\n",
    "                        df.loc[index, \"s.includedEndemicMXSpp\"] = 1\n",
    "    \n",
    "    df.loc[index, \"t.species\"] = \",\".join(result_species)\n",
    "    df.loc[index, \"t.endemicMXspecies\"] = \",\".join(result_species_endemic)\n",
    "\n",
    "df.to_csv(file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd67be7cde95a82f2c6722b61600a7fc228d2c9cced8586fb06f345deb16a218"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "dd67be7cde95a82f2c6722b61600a7fc228d2c9cced8586fb06f345deb16a218"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
